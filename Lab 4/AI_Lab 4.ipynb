{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Lab 4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtT+37sDIWUdBNR1qaDlBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishav-Chowdhury/Artificial-Intelligence-Coursework/blob/main/AI_Lab%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N51NAdDhnaj",
        "outputId": "b728f184-5a2d-4331-cb3c-c14790723b2f"
      },
      "source": [
        "#BFS Algorithm\n",
        "\n",
        "graph={'A':['B','C','E'], \n",
        "       'B':['A','D','E'], \n",
        "       'C':['A','F','G'], \n",
        "       'D':['B'], \n",
        "       'E':['A','B','D'], \n",
        "       'F':['C'], 'G':['C']} \n",
        "\n",
        "def bfs(graph,initial): \n",
        "  visited=[] \n",
        "  queue=[initial] \n",
        "  while queue: \n",
        "    node=queue.pop(0) \n",
        "    if node not in visited: \n",
        "      visited.append(node) \n",
        "      neighbours=graph[node] \n",
        "      for neighbour in neighbours: \n",
        "        queue.append(neighbour) \n",
        "  return visited\n",
        "\n",
        "print(bfs(graph,'A'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'C', 'E', 'D', 'F', 'G']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPnv_7RjiMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b925f67d-f1f6-41a3-ea97-6c22ce6e14da"
      },
      "source": [
        "# Web crawling using BFS\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "def my_crawler(seed):\n",
        "#List_of_links is store links that are extracted on a page\n",
        " List_of_links = []\n",
        " parent_list = []\n",
        " child_list = []\n",
        " parent_list.append(seed)\n",
        " count = 1\n",
        " depth = 1\n",
        " x =0\n",
        " while (len(parent_list) < 50):\n",
        " \t#print(\"Depth :\",depth)\n",
        " \tif (depth >= 7):\n",
        " \t break\n",
        " \tList_of_links = go_crawl(parent_list[x])\n",
        " \tfor i in List_of_links:\n",
        " \t\tif i not in child_list:\n",
        " \t\t\tchild_list.append(i)\n",
        " \tfor j in child_list:\n",
        " \t\tif j not in parent_list and len(parent_list) < 50:\n",
        " \t    \t parent_list.append(j)\n",
        " \t    \t count = count+1\n",
        " \tif (len(child_list) == count):\n",
        " \t\tchild_list = []\n",
        " \t\tdepth = depth + 1\n",
        " \t\t#print(\"Depth :\",depth)\n",
        " \tx = x + 1\t\n",
        "\n",
        " #Writing to file\n",
        " number = 1\n",
        " f = open('BFS-WC-URLs.txt', 'w')\n",
        " for i in parent_list:\n",
        "     row =  str(number) + \" \" + str(i) + \"\\n\"\n",
        "     print(row)\n",
        "     f.write(row)\n",
        "     number += 1\n",
        " f.close()\n",
        "\n",
        "\n",
        "def go_crawl(url):\n",
        " #Respecting the politeness policy\n",
        " time.sleep(1)\n",
        " wikistring = \"https://en.wikipedia.org\"\n",
        " totallinks, child_links = [], []\n",
        " seedinfo = requests.get(url)\n",
        " raw_data = seedinfo.text\n",
        " soup = BeautifulSoup(raw_data, 'html.parser')\n",
        " body_content = soup.find('div', {'id': 'mw-content-text'})\n",
        " #Remove Thumbnails and links below images\n",
        " if len(soup.find('div',{'class':'thumbcaption'}) or ()) > 1:\n",
        "        soup.find('div', {'class':'thumbcaption'}).decompose()\n",
        " #Remove vertical Navigation box on the right\n",
        " if len(soup.find('table',{'class':'vertical-navbox nowraplinks hlist'}) or ()) > 1:\n",
        "        soup.find('table', {'class':'vertical-navbox nowraplinks hlist'}).decompose()\n",
        " #Remove vertical Navigation box on the right\n",
        " if len(soup.find('table',{'class':'vertical-navbox nowraplinks'}) or ()) > 1:\n",
        "        soup.find('table', {'class':'vertical-navbox nowraplinks'}).decompose()\n",
        " #Removing refrences\n",
        " if len(soup.find('ol', class_='references') or ()) > 1:\n",
        " \tsoup.find('ol', class_='references').decompose()\n",
        "\n",
        " for link in body_content.find_all('a', {'href': re.compile(\"^/wiki\")}):\n",
        "     if ':' not in link.get('href'):\n",
        "         link_text = wikistring + link.get('href')\n",
        "         refine_text = link_text.split('#')\n",
        "         totallinks.append(str(refine_text[0]))\n",
        " for i in totallinks:\n",
        "    if i not in child_links:\n",
        "        if len(i) > 1:\n",
        "            child_links.append(i)\n",
        " return child_links\n",
        "\n",
        "\n",
        "my_crawler('https://en.wikipedia.org/wiki/Tropical_cyclone')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 https://en.wikipedia.org/wiki/Tropical_cyclone\n",
            "\n",
            "2 https://en.wikipedia.org/wiki/Hurricane_(disambiguation)\n",
            "\n",
            "3 https://en.wikipedia.org/wiki/Hurricane_No._1\n",
            "\n",
            "4 https://en.wikipedia.org/wiki/Weather\n",
            "\n",
            "5 https://en.wikipedia.org/wiki/Season\n",
            "\n",
            "6 https://en.wikipedia.org/wiki/Winter\n",
            "\n",
            "7 https://en.wikipedia.org/wiki/Spring_(season)\n",
            "\n",
            "8 https://en.wikipedia.org/wiki/Summer\n",
            "\n",
            "9 https://en.wikipedia.org/wiki/Autumn\n",
            "\n",
            "10 https://en.wikipedia.org/wiki/Tropics\n",
            "\n",
            "11 https://en.wikipedia.org/wiki/Dry_season\n",
            "\n",
            "12 https://en.wikipedia.org/wiki/Harmattan\n",
            "\n",
            "13 https://en.wikipedia.org/wiki/Wet_season\n",
            "\n",
            "14 https://en.wikipedia.org/wiki/Storm\n",
            "\n",
            "15 https://en.wikipedia.org/wiki/Cloud\n",
            "\n",
            "16 https://en.wikipedia.org/wiki/Cumulonimbus_cloud\n",
            "\n",
            "17 https://en.wikipedia.org/wiki/Arcus_cloud\n",
            "\n",
            "18 https://en.wikipedia.org/wiki/Downburst\n",
            "\n",
            "19 https://en.wikipedia.org/wiki/Microburst\n",
            "\n",
            "20 https://en.wikipedia.org/wiki/Heat_burst\n",
            "\n",
            "21 https://en.wikipedia.org/wiki/Derecho\n",
            "\n",
            "22 https://en.wikipedia.org/wiki/Lightning\n",
            "\n",
            "23 https://en.wikipedia.org/wiki/Volcanic_lightning\n",
            "\n",
            "24 https://en.wikipedia.org/wiki/Thunderstorm\n",
            "\n",
            "25 https://en.wikipedia.org/wiki/Air-mass_thunderstorm\n",
            "\n",
            "26 https://en.wikipedia.org/wiki/Thundersnow\n",
            "\n",
            "27 https://en.wikipedia.org/wiki/Dry_thunderstorm\n",
            "\n",
            "28 https://en.wikipedia.org/wiki/Mesocyclone\n",
            "\n",
            "29 https://en.wikipedia.org/wiki/Supercell\n",
            "\n",
            "30 https://en.wikipedia.org/wiki/Tornado\n",
            "\n",
            "31 https://en.wikipedia.org/wiki/Anticyclonic_tornado\n",
            "\n",
            "32 https://en.wikipedia.org/wiki/Landspout\n",
            "\n",
            "33 https://en.wikipedia.org/wiki/Waterspout\n",
            "\n",
            "34 https://en.wikipedia.org/wiki/Dust_devil\n",
            "\n",
            "35 https://en.wikipedia.org/wiki/Fire_whirl\n",
            "\n",
            "36 https://en.wikipedia.org/wiki/Anticyclone\n",
            "\n",
            "37 https://en.wikipedia.org/wiki/Cyclone\n",
            "\n",
            "38 https://en.wikipedia.org/wiki/Polar_low\n",
            "\n",
            "39 https://en.wikipedia.org/wiki/Extratropical_cyclone\n",
            "\n",
            "40 https://en.wikipedia.org/wiki/European_windstorm\n",
            "\n",
            "41 https://en.wikipedia.org/wiki/Nor%27easter\n",
            "\n",
            "42 https://en.wikipedia.org/wiki/Subtropical_cyclone\n",
            "\n",
            "43 https://en.wikipedia.org/wiki/Atlantic_hurricane\n",
            "\n",
            "44 https://en.wikipedia.org/wiki/Typhoon\n",
            "\n",
            "45 https://en.wikipedia.org/wiki/Storm_surge\n",
            "\n",
            "46 https://en.wikipedia.org/wiki/Dust_storm\n",
            "\n",
            "47 https://en.wikipedia.org/wiki/Simoom\n",
            "\n",
            "48 https://en.wikipedia.org/wiki/Haboob\n",
            "\n",
            "49 https://en.wikipedia.org/wiki/Monsoon\n",
            "\n",
            "50 https://en.wikipedia.org/wiki/Amihan\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1cYBGi7k81z",
        "outputId": "fcbd6097-296d-4b27-99bb-4184c4645406"
      },
      "source": [
        "#DFS algorithm\n",
        "\n",
        "graph1={'A':['B','C','E'], \n",
        "        'B':['A','D','E'], \n",
        "        'C':['A','F','G'], \n",
        "        'D':['B'], \n",
        "        'E':['A','B','D'], \n",
        "        'F':['C'], 'G':['C']} \n",
        "\n",
        "def dfs(graph,node,visited): \n",
        "  if node not in visited: \n",
        "    visited.append(node) \n",
        "    for k in graph[node]: \n",
        "      dfs(graph,k,visited) \n",
        "  return visited\n",
        "\n",
        "visited = dfs(graph1,'A',[]) \n",
        "print(visited)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'D', 'E', 'C', 'F', 'G']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS-qeonOlSlU",
        "outputId": "cd7be7bf-b593-4387-ff64-50aba8293751"
      },
      "source": [
        "#DFS\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "def my_crawler(seed, keyword):\n",
        "#List_of_links is store links that are extracted on a page\n",
        " List_of_links = []\n",
        " parent_list = []\n",
        " child_list = []\n",
        " parent_list.append(seed)\n",
        " count = 1\n",
        " depth = 1\n",
        " x =0\n",
        " while (len(parent_list) < 50):\n",
        " \tif (depth >= 6):\n",
        " \t break\n",
        " \tList_of_links = go_crawl(parent_list[x], keyword)\n",
        " \tfor i in List_of_links:\n",
        " \t\tif i not in child_list:\n",
        " \t\t\tchild_list.append(i)\n",
        " \tfor j in child_list:\n",
        " \t\tif j not in parent_list and len(parent_list) < 50:\n",
        " \t    \t parent_list.append(j)\n",
        " \t    \t count = count+1\n",
        " \tif (len(child_list) == count):\n",
        " \t\tchild_list = []\n",
        " \t\tdepth = depth + 1\n",
        " #\t\tprint(depth)\n",
        "#        print(len(parent_list))\n",
        " \tx = x + 1\t\n",
        "# print(keyword)\n",
        "# print(depth)\n",
        "\n",
        " #Writing to file\n",
        " number = 1\n",
        " f = open('DFS-WC-URLs.txt', 'w')\n",
        " for i in parent_list:\n",
        "     row =  str(number) + \" \" + str(i) + \"\\n\"\n",
        "     print(row)\n",
        "     f.write(row)\n",
        "     number += 1\n",
        " f.close()\n",
        "\n",
        "\n",
        "def go_crawl(url,keyword):\n",
        " reExp=\"^%s| %s |%s_|_%s \"%(keyword,keyword,keyword,keyword)\n",
        " Exp = re.compile(reExp,re.IGNORECASE)\n",
        " wikistring = \"https://en.wikipedia.org\"\n",
        " totallinks, child_links = [], []\n",
        " time.sleep(1)\n",
        " seedinfo = requests.get(url)\n",
        " raw_data = seedinfo.text\n",
        " soup = BeautifulSoup(raw_data, 'html.parser')\n",
        " body_content = soup.find('div', {'id': 'mw-content-text'})\n",
        " #Remove Thumbnails and links below images\n",
        " if len(soup.find('div',{'class':'thumbcaption'}) or ()) > 1:\n",
        "        soup.find('div', {'class':'thumbcaption'}).decompose()\n",
        " #Remove vertical Navigation box on the right\n",
        " if len(soup.find('table',{'class':'vertical-navbox nowraplinks hlist'}) or ()) > 1:\n",
        "        soup.find('table', {'class':'vertical-navbox nowraplinks hlist'}).decompose()\n",
        " #Remove vertical Navigation box on the right\n",
        " if len(soup.find('table',{'class':'vertical-navbox nowraplinks'}) or ()) > 1:\n",
        "        soup.find('table', {'class':'vertical-navbox nowraplinks'}).decompose()\n",
        " #Removing refrences\n",
        " if len(soup.find('ol', class_='references') or ()) > 1:\n",
        " \tsoup.find('ol', class_='references').decompose()\n",
        "\n",
        " for link in body_content.find_all('a', {'href': re.compile(\"^/wiki\")}):\n",
        "    urlString = link.get('href')\n",
        "    len1=len(Exp.findall(urlString))\n",
        "    try:\n",
        "       aTextString = link.text \n",
        "    except UnicodeEncodeError as e:\n",
        "     error = e\n",
        "    len2=len(Exp.findall(keyword))\n",
        "    if(len1 > 0) or (len2 > 0):\n",
        "        if ':' not in link.get('href'):\n",
        "            link_text = wikistring + link.get('href')\n",
        "            refine_text = link_text.split('#')\n",
        "            totallinks.append(str(refine_text[0]))\n",
        " for i in totallinks:\n",
        "    if i not in child_links:\n",
        "        if len(i) > 1:\n",
        "            child_links.append(i)\n",
        " return child_links\n",
        "\n",
        "keyword = input(\"Enter the Key that needs to be searched: \")\n",
        "my_crawler('https://en.wikipedia.org/wiki/Tropical_cyclone',keyword)\t\n",
        "Link_limit = 35"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the Key that needs to be searched: Cyclone\n",
            "1 https://en.wikipedia.org/wiki/Tropical_cyclone\n",
            "\n",
            "2 https://en.wikipedia.org/wiki/Hurricane_(disambiguation)\n",
            "\n",
            "3 https://en.wikipedia.org/wiki/Hurricane_No._1\n",
            "\n",
            "4 https://en.wikipedia.org/wiki/Weather\n",
            "\n",
            "5 https://en.wikipedia.org/wiki/Season\n",
            "\n",
            "6 https://en.wikipedia.org/wiki/Winter\n",
            "\n",
            "7 https://en.wikipedia.org/wiki/Spring_(season)\n",
            "\n",
            "8 https://en.wikipedia.org/wiki/Summer\n",
            "\n",
            "9 https://en.wikipedia.org/wiki/Autumn\n",
            "\n",
            "10 https://en.wikipedia.org/wiki/Tropics\n",
            "\n",
            "11 https://en.wikipedia.org/wiki/Dry_season\n",
            "\n",
            "12 https://en.wikipedia.org/wiki/Harmattan\n",
            "\n",
            "13 https://en.wikipedia.org/wiki/Wet_season\n",
            "\n",
            "14 https://en.wikipedia.org/wiki/Storm\n",
            "\n",
            "15 https://en.wikipedia.org/wiki/Cloud\n",
            "\n",
            "16 https://en.wikipedia.org/wiki/Cumulonimbus_cloud\n",
            "\n",
            "17 https://en.wikipedia.org/wiki/Arcus_cloud\n",
            "\n",
            "18 https://en.wikipedia.org/wiki/Downburst\n",
            "\n",
            "19 https://en.wikipedia.org/wiki/Microburst\n",
            "\n",
            "20 https://en.wikipedia.org/wiki/Heat_burst\n",
            "\n",
            "21 https://en.wikipedia.org/wiki/Derecho\n",
            "\n",
            "22 https://en.wikipedia.org/wiki/Lightning\n",
            "\n",
            "23 https://en.wikipedia.org/wiki/Volcanic_lightning\n",
            "\n",
            "24 https://en.wikipedia.org/wiki/Thunderstorm\n",
            "\n",
            "25 https://en.wikipedia.org/wiki/Air-mass_thunderstorm\n",
            "\n",
            "26 https://en.wikipedia.org/wiki/Thundersnow\n",
            "\n",
            "27 https://en.wikipedia.org/wiki/Dry_thunderstorm\n",
            "\n",
            "28 https://en.wikipedia.org/wiki/Mesocyclone\n",
            "\n",
            "29 https://en.wikipedia.org/wiki/Supercell\n",
            "\n",
            "30 https://en.wikipedia.org/wiki/Tornado\n",
            "\n",
            "31 https://en.wikipedia.org/wiki/Anticyclonic_tornado\n",
            "\n",
            "32 https://en.wikipedia.org/wiki/Landspout\n",
            "\n",
            "33 https://en.wikipedia.org/wiki/Waterspout\n",
            "\n",
            "34 https://en.wikipedia.org/wiki/Dust_devil\n",
            "\n",
            "35 https://en.wikipedia.org/wiki/Fire_whirl\n",
            "\n",
            "36 https://en.wikipedia.org/wiki/Anticyclone\n",
            "\n",
            "37 https://en.wikipedia.org/wiki/Cyclone\n",
            "\n",
            "38 https://en.wikipedia.org/wiki/Polar_low\n",
            "\n",
            "39 https://en.wikipedia.org/wiki/Extratropical_cyclone\n",
            "\n",
            "40 https://en.wikipedia.org/wiki/European_windstorm\n",
            "\n",
            "41 https://en.wikipedia.org/wiki/Nor%27easter\n",
            "\n",
            "42 https://en.wikipedia.org/wiki/Subtropical_cyclone\n",
            "\n",
            "43 https://en.wikipedia.org/wiki/Atlantic_hurricane\n",
            "\n",
            "44 https://en.wikipedia.org/wiki/Typhoon\n",
            "\n",
            "45 https://en.wikipedia.org/wiki/Storm_surge\n",
            "\n",
            "46 https://en.wikipedia.org/wiki/Dust_storm\n",
            "\n",
            "47 https://en.wikipedia.org/wiki/Simoom\n",
            "\n",
            "48 https://en.wikipedia.org/wiki/Haboob\n",
            "\n",
            "49 https://en.wikipedia.org/wiki/Monsoon\n",
            "\n",
            "50 https://en.wikipedia.org/wiki/Amihan\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
